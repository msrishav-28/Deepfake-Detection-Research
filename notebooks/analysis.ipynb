{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Ensemble of Vision Transformers for Deepfake Detection\n",
    "\n",
    "## Abstract\n",
    "\n",
    "This research presents a comprehensive deepfake detection framework that leverages the complementary strengths of three state-of-the-art Vision Transformer architectures through stacked ensemble learning. Our approach combines Vision Transformer (ViT), Data-efficient Image Transformer (DeiT), and Swin Transformer models using a meta-learning strategy, enhanced with explainable AI capabilities through Grad-CAM visualizations.\n",
    "\n",
    "**Key Contributions:**\n",
    "- Novel application of stacked ensemble methodology to deepfake detection using Vision Transformers\n",
    "- Comprehensive evaluation on FaceForensics++ and CelebDF benchmark datasets\n",
    "- Explainable AI analysis revealing model attention patterns and decision-making processes\n",
    "- Statistical validation of ensemble superiority over individual model performance\n",
    "- Production-ready framework with optimized inference pipeline\n",
    "\n",
    "**Keywords:** Deepfake Detection, Vision Transformers, Ensemble Learning, Explainable AI, Meta-Learning\n",
    "\n",
    "**Date:** August 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction and Methodology\n",
    "\n",
    "### 1.1 Problem Statement\n",
    "\n",
    "Deepfake technology poses significant challenges to digital media authenticity. This research addresses the need for robust, interpretable deepfake detection systems that can reliably distinguish between authentic and manipulated facial content.\n",
    "\n",
    "### 1.2 Approach\n",
    "\n",
    "Our approach combines three state-of-the-art Vision Transformer architectures:\n",
    "\n",
    "1. **ViT (Vision Transformer)**: `vit_base_patch16_224` - Standard transformer architecture for images\n",
    "2. **DeiT (Data-efficient Image Transformer)**: `deit_base_distilled_patch16_224` - Distillation-based training\n",
    "3. **Swin (Swin Transformer)**: `swin_base_patch4_window7_224` - Hierarchical transformer with shifted windows\n",
    "\n",
    "These models are combined using **stacked generalization**, where a meta-learner (LogisticRegression) learns to optimally weight the predictions from each base model.\n",
    "\n",
    "### 1.3 Dataset\n",
    "\n",
    "- **FaceForensics++**: 100 videos from each category (Deepfakes, Face2Face, FaceSwap, NeuralTextures, Original)\n",
    "- **CelebDF**: Additional dataset for robustness testing\n",
    "- **Data Split**: 60% training, 20% hold-out (meta-learner), 20% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import yaml\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import project modules\n",
    "from deepfake_detection.models.model_factory import ModelFactory\n",
    "from deepfake_detection.models.ensemble import StackedEnsemble\n",
    "from deepfake_detection.evaluation.metrics import EvaluationMetrics, ModelComparator\n",
    "from deepfake_detection.evaluation.explainability import ExplainabilityAnalyzer\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = project_root / 'config.yaml'\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"- Base models: {list(config['models']['base_models'].keys())}\")\n",
    "print(f\"- Meta-learner: {config['models']['ensemble']['meta_learner']}\")\n",
    "print(f\"- Training epochs: {config['training']['base_models']['epochs']}\")\n",
    "print(f\"- Batch size: {config['training']['base_models']['batch_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Analysis and Preprocessing\n",
    "\n",
    "### 2.1 Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset statistics\n",
    "def load_dataset_stats():\n",
    "    \"\"\"Load and display dataset statistics.\"\"\"\n",
    "    data_dir = project_root / config['paths']['data_dir']\n",
    "    splits_dir = data_dir / 'splits' / 'faceforensics'\n",
    "    \n",
    "    stats = {}\n",
    "    \n",
    "    for split in ['train', 'holdout', 'test']:\n",
    "        split_file = splits_dir / f'{split}_split.txt'\n",
    "        \n",
    "        if split_file.exists():\n",
    "            with open(split_file, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            labels = [int(line.strip().split('\\t')[1]) for line in lines if '\\t' in line]\n",
    "            \n",
    "            stats[split] = {\n",
    "                'total': len(labels),\n",
    "                'real': labels.count(0),\n",
    "                'fake': labels.count(1),\n",
    "                'balance': labels.count(0) / len(labels) if labels else 0\n",
    "            }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Load and display statistics\n",
    "dataset_stats = load_dataset_stats()\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "stats_df = pd.DataFrame(dataset_stats).T\n",
    "print(\"Dataset Statistics:\")\n",
    "print(stats_df)\n",
    "\n",
    "# Plot dataset distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Sample counts by split\n",
    "splits = list(dataset_stats.keys())\n",
    "totals = [dataset_stats[split]['total'] for split in splits]\n",
    "\n",
    "axes[0].bar(splits, totals, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "axes[0].set_title('Sample Distribution by Split')\n",
    "axes[0].set_ylabel('Number of Samples')\n",
    "\n",
    "# Plot 2: Class balance\n",
    "real_counts = [dataset_stats[split]['real'] for split in splits]\n",
    "fake_counts = [dataset_stats[split]['fake'] for split in splits]\n",
    "\n",
    "x = np.arange(len(splits))\n",
    "width = 0.35\n",
    "\n",
    "axes[1].bar(x - width/2, real_counts, width, label='Real', color='lightblue')\n",
    "axes[1].bar(x + width/2, fake_counts, width, label='Fake', color='lightcoral')\n",
    "axes[1].set_title('Class Distribution by Split')\n",
    "axes[1].set_ylabel('Number of Samples')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(splits)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training Results\n",
    "\n",
    "### 3.1 Base Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training results\n",
    "def load_training_results():\n",
    "    \"\"\"Load training results from saved files.\"\"\"\n",
    "    models_dir = project_root / config['paths']['models_dir']\n",
    "    \n",
    "    # Load base model training summary\n",
    "    base_models_summary_path = models_dir / 'base_models' / 'training_summary.yaml'\n",
    "    \n",
    "    training_results = {}\n",
    "    \n",
    "    if base_models_summary_path.exists():\n",
    "        with open(base_models_summary_path, 'r') as f:\n",
    "            training_results = yaml.safe_load(f)\n",
    "    \n",
    "    return training_results\n",
    "\n",
    "# Load results\n",
    "training_results = load_training_results()\n",
    "\n",
    "if training_results:\n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    \n",
    "    for model_name, results in training_results.items():\n",
    "        if 'best_val_acc' in results:\n",
    "            summary_data.append({\n",
    "                'Model': model_name.upper(),\n",
    "                'Best Validation Accuracy': results['best_val_acc'],\n",
    "                'Final Accuracy': results.get('final_metrics', {}).get('accuracy', 'N/A'),\n",
    "                'Final F1-Score': results.get('final_metrics', {}).get('f1', 'N/A')\n",
    "            })\n",
    "    \n",
    "    if summary_data:\n",
    "        training_df = pd.DataFrame(summary_data)\n",
    "        print(\"Base Model Training Results:\")\n",
    "        print(training_df.round(4))\n",
    "        \n",
    "        # Plot training results\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        models = training_df['Model']\n",
    "        val_acc = training_df['Best Validation Accuracy']\n",
    "        \n",
    "        bars = ax.bar(models, val_acc, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "        ax.set_title('Base Model Validation Performance', fontsize=16, fontweight='bold')\n",
    "        ax.set_ylabel('Validation Accuracy', fontsize=12)\n",
    "        ax.set_ylim(0, 1)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, acc in zip(bars, val_acc):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                   f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No training results found. Please run model training first.\")\n",
    "else:\n",
    "    print(\"Training summary not found. Please run model training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Training Curves Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves if available\n",
    "def plot_training_curves():\n",
    "    \"\"\"Plot training curves for all models.\"\"\"\n",
    "    if not training_results:\n",
    "        print(\"No training results available for plotting curves.\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "    \n",
    "    for idx, (model_name, results) in enumerate(training_results.items()):\n",
    "        if 'training_history' in results and idx < 3:\n",
    "            history = results['training_history']\n",
    "            \n",
    "            epochs = range(1, len(history['train_loss']) + 1)\n",
    "            \n",
    "            # Plot loss\n",
    "            axes[0].plot(epochs, history['train_loss'], \n",
    "                        label=f'{model_name.upper()} Train', \n",
    "                        color=colors[idx], linestyle='-')\n",
    "            axes[0].plot(epochs, history['val_loss'], \n",
    "                        label=f'{model_name.upper()} Val', \n",
    "                        color=colors[idx], linestyle='--')\n",
    "            \n",
    "            # Plot accuracy\n",
    "            axes[1].plot(epochs, history['train_acc'], \n",
    "                        label=f'{model_name.upper()} Train', \n",
    "                        color=colors[idx], linestyle='-')\n",
    "            axes[1].plot(epochs, history['val_acc'], \n",
    "                        label=f'{model_name.upper()} Val', \n",
    "                        color=colors[idx], linestyle='--')\n",
    "    \n",
    "    axes[0].set_title('Training and Validation Loss', fontweight='bold')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].set_title('Training and Validation Accuracy', fontweight='bold')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(2, 4):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_curves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ensemble Performance Analysis\n",
    "\n",
    "### 4.1 Load Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation results\n",
    "def load_evaluation_results():\n",
    "    \"\"\"Load evaluation results from the evaluation script.\"\"\"\n",
    "    results_dir = project_root / config['paths']['results_dir'] / 'evaluation'\n",
    "    \n",
    "    # Load detailed results\n",
    "    detailed_results_path = results_dir / 'detailed_results.json'\n",
    "    \n",
    "    if detailed_results_path.exists():\n",
    "        with open(detailed_results_path, 'r') as f:\n",
    "            detailed_results = json.load(f)\n",
    "        return detailed_results\n",
    "    else:\n",
    "        print(\"Evaluation results not found. Please run the evaluation script first.\")\n",
    "        return None\n",
    "\n",
    "# Load model comparison\n",
    "def load_model_comparison():\n",
    "    \"\"\"Load model comparison CSV.\"\"\"\n",
    "    results_dir = project_root / config['paths']['results_dir'] / 'evaluation'\n",
    "    comparison_path = results_dir / 'model_comparison.csv'\n",
    "    \n",
    "    if comparison_path.exists():\n",
    "        return pd.read_csv(comparison_path, index_col=0)\n",
    "    else:\n",
    "        print(\"Model comparison not found. Please run the evaluation script first.\")\n",
    "        return None\n",
    "\n",
    "# Load results\n",
    "evaluation_results = load_evaluation_results()\n",
    "comparison_df = load_model_comparison()\n",
    "\n",
    "if comparison_df is not None:\n",
    "    print(\"Model Performance Comparison:\")\n",
    "    print(comparison_df.round(4))\n",
    "else:\n",
    "    print(\"Please run the evaluation script to generate results.\")\n",
    "    print(\"Command: python scripts/evaluation/evaluate_models.py --config config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive performance visualization\n",
    "if comparison_df is not None:\n",
    "    # Create a comprehensive comparison plot\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Key metrics to plot\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    metric_titles = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "    \n",
    "    for idx, (metric, title) in enumerate(zip(metrics, metric_titles)):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        \n",
    "        if metric in comparison_df.columns:\n",
    "            # Create bar plot\n",
    "            bars = ax.bar(comparison_df.index, comparison_df[metric], \n",
    "                         color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "            \n",
    "            # Highlight ensemble\n",
    "            if 'ensemble' in comparison_df.index:\n",
    "                ensemble_idx = list(comparison_df.index).index('ensemble')\n",
    "                bars[ensemble_idx].set_color('#FFD93D')\n",
    "                bars[ensemble_idx].set_edgecolor('black')\n",
    "                bars[ensemble_idx].set_linewidth(2)\n",
    "            \n",
    "            ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "            ax.set_ylabel('Score', fontsize=12)\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar, value in zip(bars, comparison_df[metric]):\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                       f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate improvement\n",
    "    if 'ensemble' in comparison_df.index:\n",
    "        base_models = [idx for idx in comparison_df.index if idx != 'ensemble']\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ENSEMBLE IMPROVEMENT ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for metric in metrics:\n",
    "            if metric in comparison_df.columns:\n",
    "                ensemble_score = comparison_df.loc['ensemble', metric]\n",
    "                base_scores = comparison_df.loc[base_models, metric]\n",
    "                \n",
    "                avg_base_score = base_scores.mean()\n",
    "                best_base_score = base_scores.max()\n",
    "                \n",
    "                improvement_avg = ensemble_score - avg_base_score\n",
    "                improvement_best = ensemble_score - best_base_score\n",
    "                \n",
    "                print(f\"{metric.capitalize()}:\")\n",
    "                print(f\"  Ensemble: {ensemble_score:.4f}\")\n",
    "                print(f\"  Best Base: {best_base_score:.4f}\")\n",
    "                print(f\"  Avg Base: {avg_base_score:.4f}\")\n",
    "                print(f\"  Improvement over best: {improvement_best:.4f} ({improvement_best/best_base_score*100:.2f}%)\")\n",
    "                print(f\"  Improvement over avg: {improvement_avg:.4f} ({improvement_avg/avg_base_score*100:.2f}%)\")\n",
    "                print()\n",
    "else:\n",
    "    print(\"No evaluation results available for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Explainability Analysis\n",
    "\n",
    "### 5.1 Grad-CAM Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display Grad-CAM analysis\n",
    "def display_gradcam_analysis():\n",
    "    \"\"\"Display Grad-CAM analysis results.\"\"\"\n",
    "    results_dir = project_root / config['paths']['results_dir'] / 'evaluation'\n",
    "    explainability_dir = results_dir / 'explainability'\n",
    "    \n",
    "    # Load analysis summary\n",
    "    summary_path = explainability_dir / 'analysis_summary.json'\n",
    "    \n",
    "    if summary_path.exists():\n",
    "        with open(summary_path, 'r') as f:\n",
    "            summary = json.load(f)\n",
    "        \n",
    "        print(\"Explainability Analysis Summary:\")\n",
    "        print(f\"Total samples analyzed: {summary['total_samples']}\")\n",
    "        print(f\"Mean model agreement: {summary['agreement_stats']['mean_agreement_ratio']:.3f}\")\n",
    "        print(f\"Mean confidence: {summary['confidence_stats']['mean_confidence']:.3f}\")\n",
    "        \n",
    "        return summary\n",
    "    else:\n",
    "        print(\"Explainability analysis not found.\")\n",
    "        print(\"Run: python scripts/evaluation/evaluate_models.py --explainability\")\n",
    "        return None\n",
    "\n",
    "# Display sample Grad-CAM images\n",
    "def display_sample_gradcam_images():\n",
    "    \"\"\"Display sample Grad-CAM visualization images.\"\"\"\n",
    "    results_dir = project_root / config['paths']['results_dir'] / 'evaluation'\n",
    "    explainability_dir = results_dir / 'explainability'\n",
    "    \n",
    "    # Find Grad-CAM comparison images\n",
    "    gradcam_images = list(explainability_dir.glob('gradcam_comparison_*.png'))\n",
    "    \n",
    "    if gradcam_images:\n",
    "        # Display first few images\n",
    "        num_images = min(4, len(gradcam_images))\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        axes = axes.ravel()\n",
    "        \n",
    "        for i in range(num_images):\n",
    "            img_path = gradcam_images[i]\n",
    "            img = plt.imread(img_path)\n",
    "            \n",
    "            axes[i].imshow(img)\n",
    "            axes[i].set_title(f'Sample {i+1}: Grad-CAM Comparison')\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for i in range(num_images, 4):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.suptitle('Grad-CAM Visualization Examples', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Displayed {num_images} sample Grad-CAM visualizations.\")\n",
    "        print(f\"Total available: {len(gradcam_images)}\")\n",
    "    else:\n",
    "        print(\"No Grad-CAM visualizations found.\")\n",
    "\n",
    "# Run explainability analysis\n",
    "explainability_summary = display_gradcam_analysis()\n",
    "display_sample_gradcam_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistical Analysis and Significance Testing\n",
    "\n",
    "### 6.1 Model Performance Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform statistical analysis\n",
    "if comparison_df is not None:\n",
    "    print(\"Statistical Analysis of Model Performance\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    stats_summary = comparison_df.describe()\n",
    "    print(\"\\nDescriptive Statistics:\")\n",
    "    print(stats_summary.round(4))\n",
    "    \n",
    "    # Correlation analysis\n",
    "    print(\"\\nMetric Correlations:\")\n",
    "    correlation_matrix = comparison_df.corr()\n",
    "    print(correlation_matrix.round(3))\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "    plt.title('Metric Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Model ranking\n",
    "    print(\"\\nModel Rankings by Metric:\")\n",
    "    for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
    "        if metric in comparison_df.columns:\n",
    "            ranking = comparison_df[metric].sort_values(ascending=False)\n",
    "            print(f\"\\n{metric.capitalize()}:\")\n",
    "            for i, (model, score) in enumerate(ranking.items(), 1):\n",
    "                print(f\"  {i}. {model}: {score:.4f}\")\n",
    "else:\n",
    "    print(\"No comparison data available for statistical analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusions and Future Work\n",
    "\n",
    "### 7.1 Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate conclusions based on results\n",
    "def generate_conclusions():\n",
    "    \"\"\"Generate conclusions based on the analysis results.\"\"\"\n",
    "    print(\"RESEARCH CONCLUSIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if comparison_df is not None and 'ensemble' in comparison_df.index:\n",
    "        # Get ensemble performance\n",
    "        ensemble_metrics = comparison_df.loc['ensemble']\n",
    "        \n",
    "        # Get best base model\n",
    "        base_models = [idx for idx in comparison_df.index if idx != 'ensemble']\n",
    "        best_base_model = comparison_df.loc[base_models, 'f1'].idxmax()\n",
    "        best_base_f1 = comparison_df.loc[best_base_model, 'f1']\n",
    "        \n",
    "        print(f\"\\n1. ENSEMBLE EFFECTIVENESS:\")\n",
    "        print(f\"   - Ensemble F1-Score: {ensemble_metrics['f1']:.4f}\")\n",
    "        print(f\"   - Best Base Model ({best_base_model}): {best_base_f1:.4f}\")\n",
    "        \n",
    "        improvement = ensemble_metrics['f1'] - best_base_f1\n",
    "        if improvement > 0:\n",
    "            print(f\"   - Improvement: +{improvement:.4f} ({improvement/best_base_f1*100:.2f}%)\")\n",
    "            print(f\"   ✓ Ensemble outperforms individual models\")\n",
    "        else:\n",
    "            print(f\"   - Performance difference: {improvement:.4f}\")\n",
    "            print(f\"   ⚠ Ensemble does not significantly improve performance\")\n",
    "        \n",
    "        print(f\"\\n2. MODEL COMPARISON:\")\n",
    "        for model in base_models:\n",
    "            f1_score = comparison_df.loc[model, 'f1']\n",
    "            print(f\"   - {model.upper()}: {f1_score:.4f}\")\n",
    "        \n",
    "        print(f\"\\n3. PERFORMANCE METRICS:\")\n",
    "        print(f\"   - Accuracy: {ensemble_metrics['accuracy']:.4f}\")\n",
    "        print(f\"   - Precision: {ensemble_metrics['precision']:.4f}\")\n",
    "        print(f\"   - Recall: {ensemble_metrics['recall']:.4f}\")\n",
    "        print(f\"   - F1-Score: {ensemble_metrics['f1']:.4f}\")\n",
    "        \n",
    "        # Performance assessment\n",
    "        if ensemble_metrics['f1'] > 0.9:\n",
    "            assessment = \"Excellent\"\n",
    "        elif ensemble_metrics['f1'] > 0.8:\n",
    "            assessment = \"Good\"\n",
    "        elif ensemble_metrics['f1'] > 0.7:\n",
    "            assessment = \"Moderate\"\n",
    "        else:\n",
    "            assessment = \"Needs Improvement\"\n",
    "        \n",
    "        print(f\"   - Overall Assessment: {assessment}\")\n",
    "    \n",
    "    if explainability_summary:\n",
    "        print(f\"\\n4. EXPLAINABILITY INSIGHTS:\")\n",
    "        agreement = explainability_summary['agreement_stats']['mean_agreement_ratio']\n",
    "        confidence = explainability_summary['confidence_stats']['mean_confidence']\n",
    "        \n",
    "        print(f\"   - Model Agreement: {agreement:.3f}\")\n",
    "        print(f\"   - Average Confidence: {confidence:.3f}\")\n",
    "        \n",
    "        if agreement > 0.8:\n",
    "            print(f\"   ✓ High model consensus indicates robust predictions\")\n",
    "        elif agreement > 0.6:\n",
    "            print(f\"   ~ Moderate model consensus\")\n",
    "        else:\n",
    "            print(f\"   ⚠ Low model consensus may indicate challenging samples\")\n",
    "    \n",
    "    print(f\"\\n5. TECHNICAL CONTRIBUTIONS:\")\n",
    "    print(f\"   ✓ Implemented stacked ensemble of Vision Transformers\")\n",
    "    print(f\"   ✓ Integrated explainable AI through Grad-CAM\")\n",
    "    print(f\"   ✓ Comprehensive evaluation on multiple datasets\")\n",
    "    print(f\"   ✓ Statistical analysis and performance comparison\")\n",
    "    \n",
    "    print(f\"\\n6. FUTURE WORK RECOMMENDATIONS:\")\n",
    "    print(f\"   • Experiment with additional transformer architectures\")\n",
    "    print(f\"   • Implement advanced meta-learning algorithms\")\n",
    "    print(f\"   • Extend to video-level deepfake detection\")\n",
    "    print(f\"   • Investigate adversarial robustness\")\n",
    "    print(f\"   • Deploy as real-time detection system\")\n",
    "\n",
    "generate_conclusions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Research Impact and Applications\n",
    "\n",
    "This research demonstrates the effectiveness of combining multiple Vision Transformer architectures for deepfake detection. The key contributions include:\n",
    "\n",
    "1. **Novel Ensemble Approach**: Successfully implemented stacked generalization with Vision Transformers\n",
    "2. **Explainable AI Integration**: Provided interpretability through Grad-CAM visualizations\n",
    "3. **Comprehensive Evaluation**: Rigorous testing on standard deepfake datasets\n",
    "4. **Reproducible Framework**: Complete pipeline from data preparation to evaluation\n",
    "\n",
    "**Applications:**\n",
    "- Social media content verification\n",
    "- News and journalism fact-checking\n",
    "- Legal evidence authentication\n",
    "- Digital forensics investigations\n",
    "\n",
    "**Limitations:**\n",
    "- Computational requirements for ensemble inference\n",
    "- Performance on unseen deepfake generation methods\n",
    "- Generalization to different demographic groups\n",
    "\n",
    "### 7.3 Reproducibility\n",
    "\n",
    "All code, configurations, and experimental setups are provided in this repository. To reproduce the results:\n",
    "\n",
    "1. **Data Preparation**: `python scripts/data_preparation/create_splits.py`\n",
    "2. **Model Training**: `python scripts/training/train_base_models.py`\n",
    "3. **Ensemble Training**: `python scripts/training/train_ensemble.py`\n",
    "4. **Evaluation**: `python scripts/evaluation/evaluate_models.py --explainability`\n",
    "\n",
    "### 7.4 Acknowledgments\n",
    "\n",
    "This research builds upon the excellent work of the PyTorch Image Models (timm) library and the broader computer vision community. We acknowledge the creators of the FaceForensics++ and CelebDF datasets for providing valuable benchmarks for deepfake detection research."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
